{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Spooky Author Identification (GloVe + LSTM)",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'spooky-author-identification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F7516%2F44045%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240816%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240816T071022Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9ee0f6dac4e58828a21d72efbb40eaf703d4c65b1e805fd6871950627bd0476cd93648bf764efda880d3d7ad3b8787daf36968cfaa70b9d58b41ac07e5f7373a8b320a50e2f2948dccfd80dc9658b85be2fa3ab25903a0e7aa1c39a61dec90923a6ef41623f0c61cd06f58b0563d4e81a5aa09b011e997c59fb847002b856f516c5404219a01ed654f65dc12da42ac6e04a99427bcac4b3d8c5fbbf16f5d306a0b4f8fe6768a7b64eebc849ff0190b62a7de6eee532d9dfb9ae97e7581be80939ffb1c9d3c599d53d762530dbbe60e53acfd043580063826edf7b27e24fef52f57a40cba5975be08d18474ecb627d2242bab983f3c919c0a7d6fb14352b66831,glove-global-vectors-for-word-representation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1835%2F3176%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240816%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240816T071022Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D96460a0f9e83910ff724334ee0c71a94445d1c72356b8d11dd212ab8d9e1032937570682a50abcbf42150bffb258f9f65108bf3e267b1300fd57c70560854d57b2a6ecec916dbff2770627140cf8bed6c031d0eb02bbd8cb3898643e2422104aeb06661ced5a3238c2816780f6a6db0153e1746b271e80631147846bbb78223721d5b2a9f13ab87d1502e0b33d1e07e285df927ba0dbe2480703a70e25d907a7e195b63be4e72652bda18d34a2cfbe8a0ae64718f4ca7b9c88861ca700c67cbc13a7885dd387cf33369ec51e58b35c15d02314eec12a19ce4c282a13ebc92e9999b1281e48bb6551157d9e10a4c1de93766c48d282d0a7d09f0eb95109b56a70'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "b5MXpu48Benx"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center> <span style = \"font-family: Babas; font-size: 2em;\"> Spooky Author Identification </span> </center></h2>\n",
        "<h4><center> <span style = \"font-family: Babas; font-size: 2em; font-style: italic\"> With GloVe and LSTM </span> </center></h4>\n",
        "<h4><center> <span style = \"font-family: Babas; font-size: 2em;\"> Sugata Ghosh </span> </center></h4>"
      ],
      "metadata": {
        "id": "2Gyk2pOABen1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Overview\n",
        "\n",
        "Suppose that we are given a specific text and we only know that the author of the text is one among [**Edgar Allan Poe**](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) $(\\text{EAP})$, [**H. P. Lovecraft**](https://en.wikipedia.org/wiki/H._P._Lovecraft) $(\\text{HPL})$ and [**Mary Shelley**](https://en.wikipedia.org/wiki/Mary_Shelley) $(\\text{MWS})$. How do we predict who wrote the text? More specifically, how to predict the probability that the given text is written by Edgar Allan Poe, and the same for the other two authors?\n",
        "\n",
        "In this work, we have a large dataset of texts labeled with the true author, who is one among $\\text{EAP}$, $\\text{HPL}$ and $\\text{MWS}$. The objective is to train a model to predict probabilities that a given new text is written by $X$, where $X$ = $\\text{EAP}$, $\\text{HPL}$ and $\\text{MWS}$. We assume that the new text is indeed written by one of the authors, so that the three probabilities add up to $1$. This immediately helps us in classifying the given text as written by a specific author, for instance, we can choose the author with the highest probability of writing the text as a prediction.\n",
        "\n",
        "We use this problem to illustrate the use of two relevant techniques: [**GloVe**](https://en.wikipedia.org/wiki/GloVe) model for [**word vectorizations**](https://en.wikipedia.org/wiki/Word_embedding) and [**long short-term memory**](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) [**neural network**](https://en.wikipedia.org/wiki/Artificial_neural_network) for **model building**. The steps in this notebook towards the mentioned objective are as follows:\n",
        "\n",
        "- We define the `multiclass_log_loss` function, which takes in a matrix of binarized true target classes `y_true_binarized`, a matrix of predicted class probabilities `y_pred_probabilities` and a clipping parameter `epsilon`, and produce the multiclass version of the [**log loss**](https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) metric between `y_true_binarized` and `y_pred_probabilities`. To utilize this function as [**loss**](https://keras.io/api/losses/) in **model compilation**, we use [**TensorFlow**](https://en.wikipedia.org/wiki/TensorFlow) and **Keras backend** functions to write it, instead of the standard [**NumPy**](https://en.wikipedia.org/wiki/NumPy) functions.\n",
        "\n",
        "- We [**split the data**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) in $80:20$ ratio (the [**training set**](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) consisting of $80\\%$ data, and the [**validation set**](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) consisting of the rest). We [**stratify**](https://en.wikipedia.org/wiki/Stratified_sampling) the split using the labels, so that the proportion of each label remains roughly the same in the training set and the validation set.\n",
        "\n",
        "- We encode the labels $\\text{EAP}$, $\\text{HPL}$ and $\\text{MWS}$ using a dictionary and map them to integer values $0$, $1$ and $2$, respectively; and convert the integer **label vectors** to **binary class matrices**, each row of which represents a [**one-hot**](https://en.wikipedia.org/wiki/One-hot) vector, corresponding to an integer component of the label vector.\n",
        "\n",
        "- We fit [**Keras tokenizer**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) on the combined list of texts from the training set and the validation set. The obtained words are then indexed by employing the **word_index** method; convert the texts to sequences of integers using the **texts_to_sequences** method; and use the [**pad_sequences**](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function of [**Keras**](https://en.wikipedia.org/wiki/Keras) to pad the sequences to a maximum length to be equal to the smallest integer greater than $m + 2s$, where $m$ and $s$ respectively denote the [**mean**](https://en.wikipedia.org/wiki/Mean) and [**standard deviation**](https://en.wikipedia.org/wiki/Standard_deviation) of the text lengths from the combined set of texts from the training set and the validation set. We construct a matrix of vector representations of the words found in the training set and the validation set by mapping the words to a $100$-dimensional vector space through **GloVe embedding**.\n",
        "\n",
        "- We build a [**sequential model**](https://www.tensorflow.org/guide/keras/sequential_model) consisting of an [**embedding layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) with weights provided by the matrix of word vectors, constructed previously; a [**SpatialDropout1D layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D); an [**LSTM layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) with number of units same as the length of the GloVe vectors; two [**dense**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) **hidden layers** with [**ReLU**](https://www.tensorflow.org/api_docs/python/tf/nn/relu) [**activation function**](https://en.wikipedia.org/wiki/Activation_function), each followed by a **dropout layer**; and an **output layer** of three [**neurons**](https://en.wikipedia.org/wiki/Artificial_neuron), corresponding to the three probabilities for the three authors, with [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) activation function. The model is compiled with the manually defined `multiclass_log_loss` function as **loss** and the [**Adam**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) [**optimizer**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/) with an initial learning rate of $0.001$, which is then regulated by a [**learning rate scheduler**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) **callback**, which employs a manually defined [**schedule**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) function `scheduler_modified_exponential` to update the [**learning rate**](https://en.wikipedia.org/wiki/Learning_rate) for the optimizer at each epoch.\n",
        "\n",
        "- We fit the model on the padded sequences generated from the training texts and the binary class matrix generated from the training labels for a set number of epochs. The training loss and the validation loss is monitored at each epoch and we stop the training procedure once the validation loss stops improving via an [**early stopping**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) [**callback**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/). We produce a plot depicting how the training loss and the validation loss evolved over epochs, giving an overall picture of the model building procedure.\n",
        "\n",
        "- We employ the trained model to predict the probabilities of the texts, in both the training set and the validation set, being written by the three authors and obtain a training log loss of $0.391$ and validation log loss of $0.581$. The predicted probabilities are then converted to labels by picking the **mode** and we get a training [**accuracy**](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) of $0.846$ and validation accuracy of $0.764$. Note that these results are not exactly reproducible as there are inherent randomness in the modeling procedure. Finally, a complete picture of the performance of the trained model on the validation set, in the context of the task of classifying the texts as written by one of the three authors, is provided through a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
        "---"
      ],
      "metadata": {
        "id": "D4rgzxu3Ben2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Contents\n",
        "\n",
        "- [**Data**](#Data)\n",
        "- [**Project Objective**](#Project-Objective)\n",
        "- [**Evaluation Metric**](#Evaluation-Metric)\n",
        "- [**Train-Validation Split**](#Train-Validation-Split)\n",
        "- [**Manual Encoding of Labels**](#Manual-Encoding-of-Labels)\n",
        "- [**Word Vectorization**](#Word-Vectorization)\n",
        "- [**GloVe**](#GloVe)\n",
        "- [**Label Vector $\\to$ Binary Class Matrix**](#Label-Vector-to-Binary-Class-Matrix)\n",
        "- [**Text Tokenization and Word Indexing**](#Text-Tokenization-and-Word-Indexing)\n",
        "- [**Text $\\to$ Sequence**](#Text-to-Sequence)\n",
        "- [**Sequence $\\to$ Padded Sequence**](#Sequence-to-Padded-Sequence)\n",
        "- [**Matrix of Word Vectors**](#Matrix-of-Word-Vectors)\n",
        "- [**LSTM**](#LSTM)\n",
        "- [**Model Building**](#Model-Building)\n",
        "- [**Prediction and Evaluation**](#Prediction-and-Evaluation)\n",
        "- [**Acknowledgements**](#Acknowledgements)\n",
        "- [**References**](#References)\n",
        "- [**Further Reading**](#Further-Reading)"
      ],
      "metadata": {
        "id": "VP7TN4IxBen3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "Q76Xp3-nBen3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, time, psutil, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "trusted": true,
        "id": "FOtV7kCGBen3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runtime and memory usage"
      ],
      "metadata": {
        "id": "cgKik3dtBen3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\n",
        "start = time.time()\n",
        "\n",
        "# Class representing the OS process and having memory_info() method to compute process memory usage\n",
        "process = psutil.Process(os.getpid())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:28.887495Z",
          "iopub.execute_input": "2023-05-12T12:29:28.888195Z",
          "iopub.status.idle": "2023-05-12T12:29:28.894721Z",
          "shell.execute_reply.started": "2023-05-12T12:29:28.888163Z",
          "shell.execute_reply": "2023-05-12T12:29:28.893102Z"
        },
        "trusted": true,
        "id": "1dePhTYUBen4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "The [**dataset**](https://www.kaggle.com/competitions/spooky-author-identification/data) used in the notebook is taken from the [**Spooky Author Identification**](https://www.kaggle.com/c/spooky-author-identification) competition, hosted by [**Kaggle**](https://www.kaggle.com/). In particular, we use only the training data, which are labeled, contained in the file **train.csv**. The rest of the data are not used in this notebook."
      ],
      "metadata": {
        "id": "j7srm_oFBen4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "data = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')\n",
        "print(f\"Memory usage : {data.memory_usage().sum()/(1024*1024):.2f} MB\")\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:28.902779Z",
          "iopub.execute_input": "2023-05-12T12:29:28.903117Z",
          "iopub.status.idle": "2023-05-12T12:29:29.090152Z",
          "shell.execute_reply.started": "2023-05-12T12:29:28.903088Z",
          "shell.execute_reply": "2023-05-12T12:29:29.088973Z"
        },
        "trusted": true,
        "id": "v1cEDTPMBen4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of text\n",
        "data['text'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.091832Z",
          "iopub.execute_input": "2023-05-12T12:29:29.092315Z",
          "iopub.status.idle": "2023-05-12T12:29:29.10022Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.092274Z",
          "shell.execute_reply": "2023-05-12T12:29:29.098889Z"
        },
        "trusted": true,
        "id": "9rcosz6zBen5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency comparison of labels\n",
        "plt.figure(figsize = (6, 4))\n",
        "ax = plt.subplot(1, 1, 1)\n",
        "plt.pie(\n",
        "    data['author'].value_counts(),\n",
        "    labels = data['author'].value_counts().index,\n",
        "    autopct = '%1.2f%%',\n",
        "    pctdistance = 0.8,\n",
        "    shadow = False,\n",
        "    radius = 1.3,\n",
        "    textprops = {'fontsize' : 10}\n",
        ")\n",
        "circle = plt.Circle((0, 0), 0.6, fc = 'white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(circle)\n",
        "ax.text(-0.21, -0.025, 'Labels', fontsize = 12)\n",
        "ax.set_xlabel('')\n",
        "# plt.suptitle(\"Frequency comparison of labels\", fontsize = 12)\n",
        "plt.subplots_adjust(wspace = 0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.101761Z",
          "iopub.execute_input": "2023-05-12T12:29:29.102201Z",
          "iopub.status.idle": "2023-05-12T12:29:29.308711Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.10216Z",
          "shell.execute_reply": "2023-05-12T12:29:29.30693Z"
        },
        "trusted": true,
        "id": "6DsoFd9XBen5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Objective\n",
        "\n",
        "The objective is to train an **LSTM** model with the help of **GloVe** embeddings, to predict probabilities that a given new text $T$ (which is assumed to be written by one among $\\text{EAP}$, $\\text{HPL}$ and $\\text{MWS}$), is written by $X$, where $X$ = $\\text{EAP}$, $\\text{HPL}$ and $\\text{MWS}$. Furthermore, these probabilities are to be used to predict the author of $T$, in a [**multiclass classification**](https://en.wikipedia.org/wiki/Multiclass_classification) setup.\n",
        "\n",
        "## Evaluation Metric\n",
        "\n",
        "In this notebook, we use the multiclass version of the **log loss** metric for evaluation of model performances, following the evaluation procedure of the original Kaggle competition. For [**binary classification**](https://en.wikipedia.org/wiki/Binary_classification) with a true class $y \\in \\{0, 1\\}$ and a probability estimate $p = P\\{y = 1\\}$, the log loss per sample is given by\n",
        "\n",
        "$$L_{\\log}(y, p) = - y\\log(p) - (1 - y)\\log(1 - p). \\tag{1}$$\n",
        "\n",
        "This extends to the multiclass case as follows. Let there be $N$ sample observations, each to be classified to one among $K$ possible classes. The true classes for the set of observations can be encoded as a binary indicator matrix $Y_{N \\times K}$, given by\n",
        "\n",
        "$$ y_{i,k} = \\begin{cases}\n",
        "                 1, & \\text{ if class } k \\text{ is the true class of observation } i, \\\\\n",
        "                 0, & \\text{ if class } k \\text{ is not the true class of observation } i,\n",
        "             \\end{cases} $$\n",
        "\n",
        "for $i = 1,2,\\ldots,N$. Thus, each row of the matrix $Y$ has exactly one element as $1$, the rest being $0$. Let $P_{N \\times K}$ be a matrix of probability estimates, with\n",
        "\n",
        "$$p_{i,k} = P\\{y_{i,k} = 1\\},$$\n",
        "\n",
        "for $k = 1,2,\\ldots,K$, for $i = 1,2,\\ldots,N$. Then the log loss for the set of $N$ observations is given by\n",
        "\n",
        "$$ L_{\\log}(Y, P) = - \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} y_{i,k} \\log p_{i,k}. \\tag{2}$$\n",
        "\n",
        "Note that in the binary case, we have $p_{i,0} = 1 - p_{i,1}$ and $y_{i,0} = 1 - y_{i,1}$. Expanding the inner sum over $y_{i,k} \\in \\{0,1\\}$ reduces to the expression in the right hand side of $(2)$ to the binary log loss for the set of $N$ observations. See the [**sklearn.metrics.log_loss**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) documentation for details on the implementation. For computing the multiclass log loss, given in $(2)$, we can rewrite the equation as follows\n",
        "\n",
        "$$ L_{\\log}(Y, P) = - \\frac{\\text{sum of the elements of }Y \\odot \\log P}{N}, \\tag{3}$$\n",
        "\n",
        "where $\\log P$ is the matrix of logarithms of the elements of $P$, given by\n",
        "\n",
        "$$ \\log P = \\begin{pmatrix}\n",
        "\\log p_{1,1} & \\log p_{1,2} & \\cdots & \\log p_{1,K} \\newline\n",
        "\\log p_{2,1} & \\log p_{2,2} & \\cdots & \\log p_{2,K} \\newline\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\newline\n",
        "\\log p_{N,1} & \\log p_{N,2} & \\cdots & \\log p_{N,K}\n",
        "\\end{pmatrix}, $$\n",
        "\n",
        "and the $\\odot$ operator produces the **Hadamard product**, also known as the **element-wise product** or **Schur product**, of two matrices of the same shape, given by\n",
        "\n",
        "$$ \\begin{pmatrix}\n",
        "x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\newline\n",
        "x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\newline\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\newline\n",
        "x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
        "\\end{pmatrix} \\odot \\begin{pmatrix}\n",
        "y_{1,1} & y_{1,2} & \\cdots & y_{1,n} \\newline\n",
        "y_{2,1} & y_{2,2} & \\cdots & y_{2,n} \\newline\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\newline\n",
        "y_{m,1} & y_{m,2} & \\cdots & y_{m,n}\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "x_{1,1}y_{1,1} & x_{1,2}y_{1,2} & \\cdots & x_{1,n}y_{1,n} \\newline\n",
        "x_{2,1}y_{2,1} & x_{2,2}y_{2,2} & \\cdots & x_{2,n}y_{2,n} \\newline\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\newline\n",
        "x_{m,1}y_{m,1} & x_{m,2}y_{m,2} & \\cdots & x_{m,n}y_{m,n}\n",
        "\\end{pmatrix}. $$\n",
        "\n",
        "In the next code block, we write a function computing the multiclass log loss, using the representation given in $(3)$. For this purpose, we use different functions from the [**tensorflow.keras.backend**](https://www.tensorflow.org/api_docs/python/tf/keras/backend) module.\n",
        "\n",
        "**Note:** An issue may arise if we have a predicted class probability as $0$ (since $\\log 0$ is undefined) or $1$ (since it necessitates other class probabilities to be $0$). For this reason, we clip the predicted class probabilities inside an interval $(\\epsilon, 1 - \\epsilon)$, for a small positive real number $\\epsilon$. In other words, if a probability value is less than $\\epsilon$, we substitute it with $\\epsilon$. Similarly, if a probability value is greater than $1 - \\epsilon$, we substitute it with $1 - \\epsilon$. By definition, for the interval to be nonempty, we must have $\\epsilon < 0.5$. In practice, we choose it to be much smaller, close to $0$. To summarize, we make the following transformation to each predicted class probability $p$:\n",
        "\n",
        "$$ p \\mapsto \\max{\\left(\\min{\\left(p, 1 - \\epsilon\\right)}, \\epsilon\\right)}. $$\n",
        "\n",
        "Here, the default value of $\\epsilon$ is taken to be $10^{-20}$."
      ],
      "metadata": {
        "id": "0oxaZcO-Ben5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_log_loss(y_true_binarized, y_pred_probabilities, epsilon = 1e-20):\n",
        "    \"\"\"\n",
        "    Computes the multiclass version oLf Log Loss metric\n",
        "    Args:\n",
        "        y_true_binarized (array_like, shape (m, n_class)): Matrix of binarized true target classes\n",
        "        y_pred_probabilities (array_like, shape (m, n_class)): Matrix of predicted class probabilities\n",
        "        eps (scalar): Clipping parameter for predicted class probabilities\n",
        "            Class probabilities outside the interval (eps, 1 - eps) are clipped to the nearest endpoint\n",
        "    Returns:\n",
        "        logloss (scalar): Multiclass log loss obtained from y_true and y_pred (eps-clipped)\n",
        "    \"\"\"\n",
        "    # Clipping to avoid undefined quantity log 0\n",
        "    y_pred_probabilities = K.clip(y_pred_probabilities, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Casting sum of the elements of Hadamard product in (3)\n",
        "    sum_ = tf.cast(K.sum(y_true_binarized * K.log(y_pred_probabilities)), tf.float64)\n",
        "\n",
        "    # Computing log loss\n",
        "    logloss = (-1 / len(y_true_binarized)) * sum_\n",
        "\n",
        "    return logloss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.312056Z",
          "iopub.execute_input": "2023-05-12T12:29:29.315075Z",
          "iopub.status.idle": "2023-05-12T12:29:29.330364Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.315012Z",
          "shell.execute_reply": "2023-05-12T12:29:29.327569Z"
        },
        "trusted": true,
        "id": "4kdL8XKqBen5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give an example with $N = 4$ observations and $K = 3$ classes. Given the true classes $(Y)$, the multiclass Log Loss is computed for a bad set of probability estimates $(P_{\\text{bad}})$ and a good set of probability estimates $(P_{\\text{good}})$."
      ],
      "metadata": {
        "id": "hlYTXd5-Ben6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples\n",
        "y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]])\n",
        "y_pred_bad = np.array([[0.1, 0.4, 0.5], [0.2, 0.1, 0.7], [0.6, 0.3, 0.1], [0.2, 0.2, 0.6]])\n",
        "y_pred_good = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.0, 0.9], [0.1, 0.8, 0.1]])\n",
        "\n",
        "print(f\"Log Loss for bad predictions: {multiclass_log_loss(y_true, y_pred_bad):.4f}\")\n",
        "print(f\"Log Loss for good predictions: {multiclass_log_loss(y_true, y_pred_good):.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.33222Z",
          "iopub.execute_input": "2023-05-12T12:29:29.332975Z",
          "iopub.status.idle": "2023-05-12T12:29:29.480473Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.332919Z",
          "shell.execute_reply": "2023-05-12T12:29:29.479196Z"
        },
        "trusted": true,
        "id": "HW4NyilxBen6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Validation Split\n",
        "\n",
        "The dataset is split in $80:20$ ratio, with the **training set** consisting of $80\\%$ of the data, and the **validation set** consisting of the rest. The split is stratified using the labels to keep the label proportions approximately equal in the training set and the validation set."
      ],
      "metadata": {
        "id": "eCrFX6V5Ben6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-validation split\n",
        "x, y = data['text'].values, data['author'].values\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y,\n",
        "                                                      stratify = y,\n",
        "                                                      random_state = 40,\n",
        "                                                      test_size = 0.2,\n",
        "                                                      shuffle = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.481811Z",
          "iopub.execute_input": "2023-05-12T12:29:29.482168Z",
          "iopub.status.idle": "2023-05-12T12:29:29.52136Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.48213Z",
          "shell.execute_reply": "2023-05-12T12:29:29.520295Z"
        },
        "trusted": true,
        "id": "vt14riGcBen6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Encoding of Labels\n",
        "\n",
        "We observe that the labels (authors) are [**nominal**](https://en.wikipedia.org/wiki/Nominal_category) in nature, i.e. they do not have an inherent [**order**](https://en.wikipedia.org/wiki/Order_theory). To encode the labels, we use a dictionary to map them to integer values in the following way: $\\text{EAP} \\mapsto 0$, $\\text{HPL} \\mapsto 1$ and $\\text{MWS} \\mapsto 2$."
      ],
      "metadata": {
        "id": "OftO7q52Ben6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual encoding of labels\n",
        "label_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
        "y_train = pd.Series(y_train).replace(label_dict, inplace = False).values\n",
        "y_valid = pd.Series(y_valid).replace(label_dict, inplace = False).values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.525594Z",
          "iopub.execute_input": "2023-05-12T12:29:29.525919Z",
          "iopub.status.idle": "2023-05-12T12:29:29.551954Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.525892Z",
          "shell.execute_reply": "2023-05-12T12:29:29.550816Z"
        },
        "trusted": true,
        "id": "2pVDPqloBen6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Vectorization\n",
        "\n",
        "In [**natural language processing**](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), we generally deal with text data in string format. However, machine learning models typically take vectors (arrays of numbers) as input. Thus, to apply the tools of machine learning to problems involving text data, it is crucial to come up with a strategy to convert strings/texts to numbers. **Word vectorizations** are a class of techniques where individual words are represented as real-valued vectors in a predefined [**vector space**](https://en.wikipedia.org/wiki/Vector_space). Typically, the representation encodes the meaning of the word in such a way that words that are similar in meaning have representations that are closer in the vector space."
      ],
      "metadata": {
        "id": "-PdCMaA5Ben7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe\n",
        "\n",
        "In this project, we use the **GloVe** model for word vectorization. GloVe, which stands for Global Vectors, is developed as [**an open-source project at Stanford**](https://nlp.stanford.edu/projects/glove/), launched in $2014$. It is an [**unsupervised learning**](https://en.wikipedia.org/wiki/Unsupervised_learning) [**algorithm**](https://en.wikipedia.org/wiki/Algorithm) to get vector representations for words. It works by mapping words into a vector space in such a way that the semantic similarity of two words is reflected in the distance between their respective vector representations. The model is trained on aggregated global word-word [**co-occurrence matrix**](https://en.wikipedia.org/wiki/Co-occurrence_matrix) from a corpus, and the resulting representations showcase interesting linear substructures of the vector space. As [**log-bilinear regression model**](https://jiangnanhugo.github.io/blog/log-bilinear-model) for unsupervised learning of word representations, it combines the features of two model families, namely the **global matrix factorization** and **local context window** methods. In the following code block, we load the GloVe vectors and store them in a dictionary for future use. We report the total number of word vectors found. As example, we show the vector representation of the word *the*. Observe that the word is mapped to a vector space of dimension $100$."
      ],
      "metadata": {
        "id": "39VqYlQxBen7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the GloVe vectors in a dictionary\n",
        "embed_glove = {}\n",
        "glove_6b_100d = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n",
        "for line in glove_6b_100d:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype = 'float32')\n",
        "    embed_glove[word] = vector\n",
        "glove_6b_100d.close()\n",
        "\n",
        "print(f\"Number of word vectors: {len(embed_glove)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:29.553556Z",
          "iopub.execute_input": "2023-05-12T12:29:29.553879Z",
          "iopub.status.idle": "2023-05-12T12:29:43.284125Z",
          "shell.execute_reply.started": "2023-05-12T12:29:29.553851Z",
          "shell.execute_reply": "2023-05-12T12:29:43.282844Z"
        },
        "trusted": true,
        "id": "Askcf9rsBen7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of vector representation of a word\n",
        "print(\"Word: 'the'\")\n",
        "print(\"Vector representation:\")\n",
        "print(embed_glove['the'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:43.286058Z",
          "iopub.execute_input": "2023-05-12T12:29:43.286439Z",
          "iopub.status.idle": "2023-05-12T12:29:43.294172Z",
          "shell.execute_reply.started": "2023-05-12T12:29:43.286401Z",
          "shell.execute_reply": "2023-05-12T12:29:43.293035Z"
        },
        "trusted": true,
        "id": "hz8WJbB_Ben7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimension of the word vectors\n",
        "dim_glove = len(embed_glove['the'])\n",
        "print(f\"Dimension: {dim_glove}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:43.295498Z",
          "iopub.execute_input": "2023-05-12T12:29:43.295815Z",
          "iopub.status.idle": "2023-05-12T12:29:43.309324Z",
          "shell.execute_reply.started": "2023-05-12T12:29:43.295787Z",
          "shell.execute_reply": "2023-05-12T12:29:43.308299Z"
        },
        "trusted": true,
        "id": "Ho7kVIN6Ben7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='Label-Vector-to-Binary-Class-Matrix'></a>\n",
        "## Label Vector $\\to$ Binary Class Matrix\n",
        "\n",
        "We convert the integer **label vectors** of training set and validation set to **binary class matrices**. Essentially, we map the integer labels to **one-hot** vectors. This can be achieved by encoding $0$ as $(1, 0, 0)$, $1$ as $(0, 1, 0)$ and $2$ as $(0, 0, 1)$. For example, we have\n",
        "\n",
        "$$ \\begin{pmatrix}\n",
        "0 \\newline\n",
        "1 \\newline\n",
        "2 \\newline\n",
        "1\n",
        "\\end{pmatrix} \\mapsto \\begin{pmatrix}\n",
        "1 & 0 & 0 \\newline\n",
        "0 & 1 & 0 \\newline\n",
        "0 & 0 & 1 \\newline\n",
        "0 & 1 & 0\n",
        "\\end{pmatrix} $$"
      ],
      "metadata": {
        "id": "jcY45hRZBen7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarization of the labels\n",
        "y_train_matrix = np_utils.to_categorical(y_train)\n",
        "y_valid_matrix = np_utils.to_categorical(y_valid)\n",
        "\n",
        "# Example\n",
        "print(f\"Original label: {y_train[0]}\")\n",
        "print(f\"Encoded label : {list(y_train_matrix[0])}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:43.310966Z",
          "iopub.execute_input": "2023-05-12T12:29:43.311289Z",
          "iopub.status.idle": "2023-05-12T12:29:43.322397Z",
          "shell.execute_reply.started": "2023-05-12T12:29:43.311262Z",
          "shell.execute_reply": "2023-05-12T12:29:43.321189Z"
        },
        "trusted": true,
        "id": "LSEiEnvNBen7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Tokenization and Word Indexing\n",
        "\n",
        "We fit **Keras tokenizer** on the combined list of training texts and validation texts using the **fit_on_texts** method. The words, thence obtained, are then indexed by employing the **word_index** method."
      ],
      "metadata": {
        "id": "qsXG-LzqBen7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and indexing using keras tokenizer\n",
        "token = text.Tokenizer(num_words = None)\n",
        "token.fit_on_texts(list(x_train) + list(x_valid))\n",
        "word_index = token.word_index\n",
        "\n",
        "# Example\n",
        "print(f\"Index of the word 'seven': {word_index['seven']}\")\n",
        "print(f\"Number of indexed words  : {len(word_index)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:43.32364Z",
          "iopub.execute_input": "2023-05-12T12:29:43.32398Z",
          "iopub.status.idle": "2023-05-12T12:29:44.12151Z",
          "shell.execute_reply.started": "2023-05-12T12:29:43.323954Z",
          "shell.execute_reply": "2023-05-12T12:29:44.118922Z"
        },
        "trusted": true,
        "id": "tW2QGiyZBen7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='Text-to-Sequence'></a>\n",
        "## Text $\\to$ Sequence\n",
        "\n",
        "Next, we use the **texts_to_sequences** method of **Keras tokenizer**, which is fit to the texts found in both the training set and the validation set, to convert these texts to sequences of integers. To be precise, a text, which is a sequence of words, is converted to the sequence of indices of those words. The tranformation is demonstrated in the next example and then implemented on the texts in training set and validation set."
      ],
      "metadata": {
        "id": "-z4kY7LhBen7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "text_ = 'the seven deadly sins'\n",
        "tokens = text_.split(\" \")\n",
        "sequence_ = [word_index[x] for x in tokens]\n",
        "\n",
        "print(f\"Text    : {text_}\")\n",
        "print(f\"Tokens  : {tokens}\")\n",
        "print(f\"Sequence: {sequence_}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:44.123101Z",
          "iopub.execute_input": "2023-05-12T12:29:44.123452Z",
          "iopub.status.idle": "2023-05-12T12:29:44.129531Z",
          "shell.execute_reply.started": "2023-05-12T12:29:44.123422Z",
          "shell.execute_reply": "2023-05-12T12:29:44.12859Z"
        },
        "trusted": true,
        "id": "Wemrp7ZABen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to sequence\n",
        "x_train_seq = token.texts_to_sequences(x_train)\n",
        "x_valid_seq = token.texts_to_sequences(x_valid)\n",
        "\n",
        "# Example\n",
        "print(f\"Text: {x_train[0]}\")\n",
        "print(\"Converted to sequence:\")\n",
        "print(x_train_seq[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:44.131252Z",
          "iopub.execute_input": "2023-05-12T12:29:44.131726Z",
          "iopub.status.idle": "2023-05-12T12:29:44.703705Z",
          "shell.execute_reply.started": "2023-05-12T12:29:44.131695Z",
          "shell.execute_reply": "2023-05-12T12:29:44.702577Z"
        },
        "trusted": true,
        "id": "3iRPqc11Ben8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='Sequence-to-Padded-Sequence'></a>\n",
        "## Sequence $\\to$ Padded Sequence\n",
        "\n",
        "Before we feed these sequences to the model, it is beneficial to make all the sequences in the training set and the validation set to fit a given standard length. For this purpose, it is necessary to pad or truncate some sequences. In **Keras**, the **pad_sequences** function takes a batch of input sequences and implements a technique which make the size of the sequences equal to either a pre-specified maximum length or the length of the longest individual sequence. The process involves adding zeros to a specified position of a sequence or truncating from a specified end of a sequence. Here we have set the maximum length to be equal to the smallest integer greater than $m + 2s$, where $m$ is the mean of the text lengths (training set and validation set combined) and $s$ is the standard deviation of the text lengths."
      ],
      "metadata": {
        "id": "8L6DA9ZYBen8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence padding\n",
        "len_train = [len(x_train_seq[i]) for i in range(len(x_train_seq))]\n",
        "len_valid = [len(x_valid_seq[i]) for i in range(len(x_valid_seq))]\n",
        "len_ = np.array(len_train + len_valid)\n",
        "maxlen_ = math.floor(len_.mean() + 2*len_.std()) + 1\n",
        "\n",
        "x_train_pad = sequence.pad_sequences(x_train_seq,\n",
        "                                    maxlen = maxlen_,\n",
        "                                    padding = 'pre',\n",
        "                                    truncating = 'pre',\n",
        "                                    value = 0.0)\n",
        "\n",
        "x_valid_pad = sequence.pad_sequences(x_valid_seq,\n",
        "                                    maxlen = maxlen_,\n",
        "                                    padding = 'pre',\n",
        "                                    truncating = 'pre',\n",
        "                                    value = 0.0)\n",
        "\n",
        "# Example (continued from the previous code block)\n",
        "print(x_train_pad[0])\n",
        "print(f\"Maximum length: {maxlen_}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:44.705241Z",
          "iopub.execute_input": "2023-05-12T12:29:44.706417Z",
          "iopub.status.idle": "2023-05-12T12:29:44.872455Z",
          "shell.execute_reply.started": "2023-05-12T12:29:44.706355Z",
          "shell.execute_reply": "2023-05-12T12:29:44.871314Z"
        },
        "trusted": true,
        "id": "DkXEp7UcBen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix of Word Vectors\n",
        "\n",
        "Next, we construct a matrix which contains vector representations (obtained through the GloVe model) of the words found in the training set and the validation set. In general, if we have $M$ words and their respective vector representations, each of dimension $N$, then the resulting matrix, containing word representations as rows, has dimension $M \\times N$."
      ],
      "metadata": {
        "id": "hazTTcf3Ben8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix of vector representations of the words in training set and validation set\n",
        "word_vectorization_matrix = np.zeros((len(word_index) + 1, dim_glove))\n",
        "for word, i in word_index.items():\n",
        "    word_embed_vector = embed_glove.get(word)\n",
        "    if word_embed_vector is not None:\n",
        "        word_vectorization_matrix[i] = word_embed_vector\n",
        "\n",
        "print(f\"Shape of the matrix of word vectors: {word_vectorization_matrix.shape}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:44.875111Z",
          "iopub.execute_input": "2023-05-12T12:29:44.875569Z",
          "iopub.status.idle": "2023-05-12T12:29:44.955925Z",
          "shell.execute_reply.started": "2023-05-12T12:29:44.875529Z",
          "shell.execute_reply": "2023-05-12T12:29:44.954799Z"
        },
        "trusted": true,
        "id": "ClSYtF0lBen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM\n",
        "\n",
        "The human brain learns in a cumulative manner. For instance, while studying a physics book, one learns the first chapter, and then learns the second chapter based on the understanding of the previous chapter. [**Recurrent neural networks**](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) function in a similar fashion, in the sense that they retain the previous information and use it for processing the current input data. However, in practice, general RNNs are not good at learning **long-term dependencies**. To explain this, we take an example from [**Christopher Olah**](https://colah.github.io/about.html)'s excellent blog post [**Understanding LSTM Networks**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Consider the following text:\n",
        "```\n",
        "\"I grew up in France... I speak fluent ____.\"\n",
        "```\n",
        "The goal is to predict the last word in the blank. The correct word is *French*. Now, if we only take the preceding word *fluent*, or the bigram *speak fluent*, or even the trigram *I speak fluent*, it only indicates that the word is likely to be the name of a particular language. If we need to find the specific language, then we require the word *France*, which is further away from the word to be predicted. However, the dependence of the two words is clear. This is how **long-term dependencies** work. The distance between the relevant information and the point where it is required in the context of the given problem can become very large. As this distance becomes large, RNNs may be unable to learn to connect the two entities, resulting in inaccurate predictions. The reason of this inability can be traced back to the [**vanishing gradient problem**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). To resolve this, many variants of RNN were developed. The most famous one, perhaps, is the **long short-term memory**, introduced by [**Sepp Hochreiter**](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [**Jurgen Schmidhuber**](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber) in $1997$. For a detailed treatment on how LSTM works, the reader is referred to the previously mentioned [**blog post**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by **Christopher Olah**."
      ],
      "metadata": {
        "id": "FbdQEJZ7Ben8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n",
        "We build a **sequential model** with the following layers:\n",
        "\n",
        "- An **embedding layer** with the matrix of word vectors, constructed previously, providing the weights. In the present context, it transforms each word index in a sequence obtained from a text, to the corresponding GloVe vector for the word, thus converting a sequence to a two-dimensional array_like object. The parameters in this layer are non-trainable.\n",
        "\n",
        "- A **SpatialDropout1D layer**, differing from usual [**dropout layers**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) in the fact that it drops entire slice of an input tensor (along a specific axis) instead of individual elements. For instance, in usual dropout, each of the eight elements in an input tensor with the shape $(2, 2, 2)$ may become zeroed out depending on the outcome of a random coin flip (with specified probability of getting head). There will be $8$ independent coin flips in total, and any number of values, from $0$ to $8$, may result in a zero. However, there are times when it may be necessary to drop the entire slice along axis $0$, meaning that if one element is dropped, the others along the same axis must also be dropped. The number of zeroed elements in this case can be $0$, $2$, $4$, $6$, or $8$, and the dropout only requires four independent random coin flips, as each dropped element drags its twin down axis $0$. This mechanism is particularly useful when dealing with slices that are strongly correlated and we may need to drop along specific axes to promote independence among the remaining slices.\n",
        "\n",
        "- An **LSTM layer** having number of units equal to the length of the GloVe vectors, with specified **dropout** (fraction of the units to drop for the linear transformation of the inputs) and **recurrent_dropout** (fraction of the units to drop for the linear transformation of the recurrent state)\n",
        "\n",
        "- Two **dense** **hidden layers** with **ReLU** **activation**, each followed by a **dropout layer**\n",
        "\n",
        "- An **output layer** of three [**neurons**](https://en.wikipedia.org/wiki/Artificial_neuron) (for three probabilities corresponding to the three authors) with [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) activation"
      ],
      "metadata": {
        "id": "WCOVTGfEBen8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                    dim_glove,\n",
        "                    weights = [word_vectorization_matrix],\n",
        "                    input_length = maxlen_,\n",
        "                    trainable = False))\n",
        "\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(dim_glove, dropout = 0.3, recurrent_dropout = 0.3))\n",
        "\n",
        "model.add(Dense(512, activation = 'relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(3, activation = 'softmax'))\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:44.957866Z",
          "iopub.execute_input": "2023-05-12T12:29:44.958537Z",
          "iopub.status.idle": "2023-05-12T12:29:45.633605Z",
          "shell.execute_reply.started": "2023-05-12T12:29:44.958505Z",
          "shell.execute_reply": "2023-05-12T12:29:45.632261Z"
        },
        "trusted": true,
        "id": "zkCh4PVOBen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we configure the model with the `multiclass_log_loss` function, defined in the **Evaluation Metric** section, and **Adam** **optimizer**."
      ],
      "metadata": {
        "id": "l5tnhalvBeoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compilation\n",
        "initial_learning_rate = 0.001\n",
        "model.compile(loss = multiclass_log_loss,\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = initial_learning_rate))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:45.635694Z",
          "iopub.execute_input": "2023-05-12T12:29:45.636141Z",
          "iopub.status.idle": "2023-05-12T12:29:45.663163Z",
          "shell.execute_reply.started": "2023-05-12T12:29:45.636101Z",
          "shell.execute_reply": "2023-05-12T12:29:45.661755Z"
        },
        "trusted": true,
        "id": "ZIGNU_YIBeoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **early stopping** **callback** to monitor the validation loss and stop the training once it stops improving by a specified margin for a specified number of epochs."
      ],
      "metadata": {
        "id": "WqkE-dl4BeoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping callback\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',\n",
        "                          min_delta = 0.001,\n",
        "                          patience = 20,\n",
        "                          verbose = 1,\n",
        "                          mode = 'auto',\n",
        "                          start_from_epoch = 60)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:45.664741Z",
          "iopub.execute_input": "2023-05-12T12:29:45.665154Z",
          "iopub.status.idle": "2023-05-12T12:29:45.67245Z",
          "shell.execute_reply.started": "2023-05-12T12:29:45.665116Z",
          "shell.execute_reply": "2023-05-12T12:29:45.671186Z"
        },
        "trusted": true,
        "id": "cDwZR6lcBeoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the [**learning rate scheduler**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) callback which employs a [**schedule**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) function to update the [**learning rate**](https://en.wikipedia.org/wiki/Learning_rate) for the optimizer. The schedule function takes the epoch index and the current learning rate as inputs and computes a new learning rate, which is then applied on the optimizer for the next epoch. In this work, we define the functions `scheduler_modified_linear` and `scheduler_modified_exponential` which apply the initial learning rate for a set number of epochs at the beginning and then decrease it linearly and exponentially, respectively. Mathematically, in the modified linear scheduler, we use the constant map $x \\mapsto x$ up to a certain number of epochs and thereafter use the following map to update the learning rate:\n",
        "\n",
        "$$x \\mapsto x\\left(\\frac{100 - \\text{epoch}}{100 - \\text{epoch} + 1}\\right)$$"
      ],
      "metadata": {
        "id": "640olDfDBeoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified linear schedule function\n",
        "def scheduler_modified_linear(epoch, learning_rate):\n",
        "    if epoch < 40:\n",
        "        return learning_rate\n",
        "    else:\n",
        "        return learning_rate * (100 - epoch) / (100 - epoch + 1)\n",
        "\n",
        "learning_rate, epoch, num_epochs, learning_rate_list = initial_learning_rate, 0, 100, []\n",
        "for i in range(num_epochs):\n",
        "    learning_rate = scheduler_modified_linear(epoch, learning_rate)\n",
        "    learning_rate_list.append(learning_rate)\n",
        "    epoch += 1\n",
        "plt.figure(figsize = (15, 10))\n",
        "plt.plot(learning_rate_list)\n",
        "plt.title(\"Learning rate vs Epoch\", fontsize = 14)\n",
        "plt.xlabel(\"Epoch\", fontsize = 14)\n",
        "plt.ylabel(\"Learning rate\", fontsize = 14)\n",
        "plt.savefig('learningrate_modified_linear.jpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:45.674419Z",
          "iopub.execute_input": "2023-05-12T12:29:45.674832Z",
          "iopub.status.idle": "2023-05-12T12:29:46.237099Z",
          "shell.execute_reply.started": "2023-05-12T12:29:45.674795Z",
          "shell.execute_reply": "2023-05-12T12:29:46.235949Z"
        },
        "trusted": true,
        "id": "W8aAgELRBeoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, in the modified exponential scheduler, we use the constant map $x \\mapsto x$ up to a fixed number of epochs and after that employ the following map to update the learning rate:\n",
        "\n",
        "$$x \\mapsto x\\exp{(-0.1)}$$"
      ],
      "metadata": {
        "id": "yG8_u5OOBeoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified exponential schedule function\n",
        "def scheduler_modified_exponential(epoch, learning_rate):\n",
        "    if epoch < 40:\n",
        "        return learning_rate\n",
        "    else:\n",
        "        return learning_rate * math.exp(-0.1)\n",
        "\n",
        "learning_rate, epoch, num_epochs, learning_rate_list = initial_learning_rate, 0, 100, []\n",
        "for i in range(num_epochs):\n",
        "    learning_rate = scheduler_modified_exponential(epoch, learning_rate)\n",
        "    learning_rate_list.append(learning_rate)\n",
        "    epoch += 1\n",
        "plt.figure(figsize = (15, 10))\n",
        "plt.plot(learning_rate_list)\n",
        "plt.title(\"Learning rate vs Epoch\", fontsize = 14)\n",
        "plt.xlabel(\"Epoch\", fontsize = 14)\n",
        "plt.ylabel(\"Learning rate\", fontsize = 14)\n",
        "plt.savefig('learningrate_modified_exponential.jpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:46.238922Z",
          "iopub.execute_input": "2023-05-12T12:29:46.24014Z",
          "iopub.status.idle": "2023-05-12T12:29:46.785418Z",
          "shell.execute_reply.started": "2023-05-12T12:29:46.24009Z",
          "shell.execute_reply": "2023-05-12T12:29:46.784428Z"
        },
        "trusted": true,
        "id": "C533Pt7GBeoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we employ the modified exponential scheduler, but one can experiment with the other schedule functions, including the [**built-in learning rate schedules**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) of Keras."
      ],
      "metadata": {
        "id": "0G0H5_p-BeoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate scheduler callback\n",
        "learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler_modified_exponential)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:46.786733Z",
          "iopub.execute_input": "2023-05-12T12:29:46.787208Z",
          "iopub.status.idle": "2023-05-12T12:29:46.793124Z",
          "shell.execute_reply.started": "2023-05-12T12:29:46.787176Z",
          "shell.execute_reply": "2023-05-12T12:29:46.791739Z"
        },
        "trusted": true,
        "id": "ZKCdMqb3BeoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we fit the the model on padded training sequences `x_train_pad` and binary class matrices `y_train_matrix` generated from training labels. We monitor the validation loss through padded validation sequences `x_valid_pad` and binary class matrices `y_valid_matrix` generated from validation labels, and use it to potentially stop the training early, once the set conditions in the `earlystop` callback is met."
      ],
      "metadata": {
        "id": "1JX03bctBeoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Model fitting\n",
        "history = model.fit(x_train_pad,\n",
        "                    y = y_train_matrix,\n",
        "                    batch_size = 256,\n",
        "                    epochs = 100,\n",
        "                    verbose = 0,\n",
        "                    validation_data = (x_valid_pad, y_valid_matrix),\n",
        "                    callbacks = [earlystop, learning_rate_scheduler])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:29:46.794786Z",
          "iopub.execute_input": "2023-05-12T12:29:46.795135Z",
          "iopub.status.idle": "2023-05-12T12:57:38.488299Z",
          "shell.execute_reply.started": "2023-05-12T12:29:46.795105Z",
          "shell.execute_reply": "2023-05-12T12:57:38.487417Z"
        },
        "trusted": true,
        "id": "VQjTDyJTBeoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we keep `verbose = 0` to avoid extensive numerical outputs. The relevant information on how the training loss and the validation loss evolve over epochs is depicted in the following diagram."
      ],
      "metadata": {
        "id": "6oIa8a2PBeoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of model loss\n",
        "plt.figure(figsize = (15, 10))\n",
        "plt.title('Model loss (multiclass log loss)', fontsize = 14)\n",
        "sns.lineplot(data = history.history['loss'], label = 'Train')\n",
        "sns.lineplot(data = history.history['val_loss'], label = 'Validation')\n",
        "plt.xlabel('Epoch', fontsize = 14)\n",
        "plt.ylabel('Loss', fontsize = 14)\n",
        "plt.legend()\n",
        "plt.savefig('logloss.jpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:57:38.489585Z",
          "iopub.execute_input": "2023-05-12T12:57:38.490697Z",
          "iopub.status.idle": "2023-05-12T12:58:13.348855Z",
          "shell.execute_reply.started": "2023-05-12T12:57:38.490652Z",
          "shell.execute_reply": "2023-05-12T12:58:13.347861Z"
        },
        "trusted": true,
        "id": "W_S4WudSBeoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction and Evaluation\n",
        "\n",
        "We use the trained model to predict the probabilities of texts being written by the three authors for both training texts and validation texts. Then we use the `multiclass_log_loss` function to compute the multiclass version of the log loss metric for predictions on both training set and validation set, to evaluate the performance of the model on the two sets. Alternately, we can use the `model.evaluate` function to directly compute the multiclass log loss for predictions on training set and validation set."
      ],
      "metadata": {
        "id": "xxGtS-85BeoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on the training set and the validation set\n",
        "y_train_matrix_pred = model.predict(x_train_pad)\n",
        "y_valid_matrix_pred = model.predict(x_valid_pad)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:13.354008Z",
          "iopub.execute_input": "2023-05-12T12:58:13.354622Z",
          "iopub.status.idle": "2023-05-12T12:58:24.616876Z",
          "shell.execute_reply.started": "2023-05-12T12:58:13.354591Z",
          "shell.execute_reply": "2023-05-12T12:58:24.615249Z"
        },
        "trusted": true,
        "id": "sQp81BqRBeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log loss\n",
        "logloss_train = multiclass_log_loss(y_train_matrix, y_train_matrix_pred).numpy()\n",
        "logloss_valid = multiclass_log_loss(y_valid_matrix, y_valid_matrix_pred).numpy()\n",
        "\n",
        "# Log loss (computed using model.evaluate)\n",
        "# logloss_train = model.evaluate(x_train_pad, y_train_matrix, batch_size = 256, verbose = 0)\n",
        "# logloss_valid = model.evaluate(x_valid_pad, y_valid_matrix, batch_size = 256, verbose = 0)\n",
        "\n",
        "print(f\"Training logloss  : {round(logloss_train, 3)}\")\n",
        "print(f\"Validation logloss: {round(logloss_valid, 3)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:24.619134Z",
          "iopub.execute_input": "2023-05-12T12:58:24.619944Z",
          "iopub.status.idle": "2023-05-12T12:58:24.637151Z",
          "shell.execute_reply.started": "2023-05-12T12:58:24.619898Z",
          "shell.execute_reply": "2023-05-12T12:58:24.63627Z"
        },
        "trusted": true,
        "id": "DRL6ufSrBeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model gives a probability vector for each text. We use it to classify the text as one written by a specific author from the set of three possible authors. Essentially we choose the author with highest probability of writing the particular text. For example, if the probability vector for a particular text turns out to be $(0.3, 0.4, 0.3)$, then we predict that the text is written by the second author $(\\text{HPL})$."
      ],
      "metadata": {
        "id": "Vowgsd-0BeoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting probability vectors to labels\n",
        "y_train_true = np.array([np.argmax(x) for x in y_train_matrix])\n",
        "y_valid_true = np.array([np.argmax(x) for x in y_valid_matrix])\n",
        "y_train_pred = np.array([np.argmax(x) for x in y_train_matrix_pred])\n",
        "y_valid_pred = np.array([np.argmax(x) for x in y_valid_matrix_pred])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:24.638522Z",
          "iopub.execute_input": "2023-05-12T12:58:24.639275Z",
          "iopub.status.idle": "2023-05-12T12:58:24.807624Z",
          "shell.execute_reply.started": "2023-05-12T12:58:24.639244Z",
          "shell.execute_reply": "2023-05-12T12:58:24.806391Z"
        },
        "trusted": true,
        "id": "FCwKgelYBeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification scheme based on the trained model is evaluated for both training data and validation data, in terms of the **accuracy** metric, which is simply the proportion of correct predictions. Additionally, we report the **confusion matrix** to give a complete representation of the model performance on the validation set, in context of the task of classification."
      ],
      "metadata": {
        "id": "4a-La273BeoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "match_train = (y_train_true == y_train_pred)\n",
        "match_train = np.array(list(map(lambda x: int(x == True), match_train)))\n",
        "match_valid = (y_valid_true == y_valid_pred)\n",
        "match_valid = np.array(list(map(lambda x: int(x == True), match_valid)))\n",
        "\n",
        "print(f\"Training accuracy  : {round(match_train.sum() / len(match_train), 3)}\")\n",
        "print(f\"Validation accuracy: {round(match_valid.sum() / len(match_valid), 3)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:24.808777Z",
          "iopub.execute_input": "2023-05-12T12:58:24.809598Z",
          "iopub.status.idle": "2023-05-12T12:58:24.878497Z",
          "shell.execute_reply.started": "2023-05-12T12:58:24.809557Z",
          "shell.execute_reply": "2023-05-12T12:58:24.877321Z"
        },
        "trusted": true,
        "id": "ozy15X72BeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute and print confusion matrix\n",
        "def conf_matrix(y_true, y_pred, n_class, class_names = 'default', figsize = (6.25, 5), font_scale = 1, annot_kws_size = 12):\n",
        "    if class_names == 'default':\n",
        "        class_names = np.arange(n_class)\n",
        "    tick_marks_y = np.arange(n_class) + 0.5\n",
        "    tick_marks_x = np.arange(n_class) + 0.5\n",
        "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
        "    confusion_matrix_df = pd.DataFrame(confusion_matrix, range(n_class), range(n_class))\n",
        "    plt.figure(figsize = figsize)\n",
        "    sns.set(font_scale = font_scale) # label size\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    sns.heatmap(confusion_matrix_df, annot = True, annot_kws = {\"size\": annot_kws_size}, fmt = 'd') # font size\n",
        "    plt.yticks(tick_marks_y, class_names, rotation = 'vertical')\n",
        "    plt.xticks(tick_marks_x, class_names, rotation = 'horizontal')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.grid(False)\n",
        "    plt.savefig('confmat.jpg')\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:24.879792Z",
          "iopub.execute_input": "2023-05-12T12:58:24.880127Z",
          "iopub.status.idle": "2023-05-12T12:58:24.889607Z",
          "shell.execute_reply.started": "2023-05-12T12:58:24.880098Z",
          "shell.execute_reply": "2023-05-12T12:58:24.888696Z"
        },
        "trusted": true,
        "id": "dDPHDcBgBeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "conf_matrix(y_valid_true, y_valid_pred, n_class = 3, class_names = ['EAP', 'HPL', 'MWS'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:24.891129Z",
          "iopub.execute_input": "2023-05-12T12:58:24.891597Z",
          "iopub.status.idle": "2023-05-12T12:58:25.359937Z",
          "shell.execute_reply.started": "2023-05-12T12:58:24.891564Z",
          "shell.execute_reply": "2023-05-12T12:58:25.35878Z"
        },
        "trusted": true,
        "id": "3AVU9masBeoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgements\n",
        "\n",
        "- [**Dataset**](https://www.kaggle.com/competitions/spooky-author-identification/data) provided in the Kaggle competition [**Spooky Author Identification**](https://www.kaggle.com/c/spooky-author-identification)\n",
        "- [**GloVe: Global Vectors for Word Representation**](https://www.kaggle.com/datasets/rtatman/glove-global-vectors-for-word-representation) dataset by [**Rachael Tatman**](https://www.kaggle.com/rtatman)\n",
        "- [**How to Choose a Learning Rate Scheduler for Neural Networks**](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler) by **Yi Li**\n",
        "- [**Practical Recommendations for Gradient-Based Training of Deep\n",
        "Architectures**](https://arxiv.org/pdf/1206.5533.pdf) by [**Yoshua Bengio**](https://en.wikipedia.org/wiki/Yoshua_Bengio)"
      ],
      "metadata": {
        "id": "Z70tPn3TBeoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [**Accuracy**](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy)\n",
        "- [**Activation function**](https://en.wikipedia.org/wiki/Activation_function)\n",
        "- [**Adam optimizer**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
        "- [**Algorithm**](https://en.wikipedia.org/wiki/Algorithm)\n",
        "- [**Artificial neuron**](https://en.wikipedia.org/wiki/Artificial_neuron)\n",
        "- [**Binary classification**](https://en.wikipedia.org/wiki/Binary_classification)\n",
        "- [**Callback**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/)\n",
        "- [**Confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
        "- [**Co-occurrence matrix**](https://en.wikipedia.org/wiki/Co-occurrence_matrix)\n",
        "- [**Dense layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
        "- [**Dropout layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n",
        "- [**Early stopping**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n",
        "- [**Edgar Allan Poe**](https://en.wikipedia.org/wiki/Edgar_Allan_Poe)\n",
        "- [**Embedding layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
        "- [**GloVe**](https://en.wikipedia.org/wiki/GloVe)\n",
        "- [**GloVe: Global Vectors for Word Representation**](https://nlp.stanford.edu/projects/glove/)\n",
        "- [**H. P. Lovecraft**](https://en.wikipedia.org/wiki/H._P._Lovecraft)\n",
        "- [**Jurgen Schmidhuber**](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber)\n",
        "- [**Kaggle**](https://www.kaggle.com/)\n",
        "- [**Keras**](https://en.wikipedia.org/wiki/Keras)\n",
        "- [**Keras built-in learning rate schedules**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)\n",
        "- [**Keras built-in optimizer classes**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/)\n",
        "- [**Keras tokenizer**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n",
        "- [**Learning rate**](https://en.wikipedia.org/wiki/Learning_rate)\n",
        "- [**Learning rate scheduler**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)\n",
        "- [**Log loss**](https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss)\n",
        "- [**Log-bilinear model**](https://jiangnanhugo.github.io/blog/log-bilinear-model)\n",
        "- [**Long short-term memory**](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
        "- [**Loss**](https://keras.io/api/losses/)\n",
        "- [**LSTM layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
        "- [**Mary Shelley**](https://en.wikipedia.org/wiki/Mary_Shelley)\n",
        "- [**Mean**](https://en.wikipedia.org/wiki/Mean)\n",
        "- [**Multiclass classification**](https://en.wikipedia.org/wiki/Multiclass_classification)\n",
        "- [**Natural language processing**](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
        "- [**Neural network**](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
        "- [**Nominal category**](https://en.wikipedia.org/wiki/Nominal_category)\n",
        "- [**NumPy**](https://en.wikipedia.org/wiki/NumPy)\n",
        "- [**One-hot**](https://en.wikipedia.org/wiki/One-hot)\n",
        "- [**Order theory**](https://en.wikipedia.org/wiki/Order_theory)\n",
        "- [**pad_sequences**](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences)\n",
        "- [**Recurrent neural network**](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
        "- [**ReLU**](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
        "- [**Schedules**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)\n",
        "- [**Sepp Hochreiter**](https://en.wikipedia.org/wiki/Sepp_Hochreiter)\n",
        "- [**Sequential model**](https://www.tensorflow.org/guide/keras/sequential_model)\n",
        "- [**sklearn.metrics.log_loss documentation**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n",
        "- [**sklearn.model_selection.train_test_split documentation**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "- [**Softmax function**](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "- [**SpatialDropout1D layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D)\n",
        "- [**Standard deviation**](https://en.wikipedia.org/wiki/Standard_deviation)\n",
        "- [**Stratified sampling**](https://en.wikipedia.org/wiki/Stratified_sampling)\n",
        "- [**TensorFlow**](https://en.wikipedia.org/wiki/TensorFlow)\n",
        "- [**tensorflow.keras.backend module**](https://www.tensorflow.org/api_docs/python/tf/keras/backend)\n",
        "- [**Training, validation, and test data sets**](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)\n",
        "- [**Unsupervised learning**](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
        "- [**Vanishing gradient problem**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
        "- [**Vector space**](https://en.wikipedia.org/wiki/Vector_space)\n",
        "- [**Word embedding**](https://en.wikipedia.org/wiki/Word_embedding)"
      ],
      "metadata": {
        "id": "6JLW1OG6BeoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "- [**GloVe: Global Vectors for Word Representation**](https://nlp.stanford.edu/pubs/glove.pdf), by [**Jeffrey Pennington**](https://nlp.stanford.edu/~jpennin/), [**Richard Socher**](https://www.socher.org/) and [**Christopher D. Manning**](https://nlp.stanford.edu/~manning/)\n",
        "- [**Glove Research Paper Clearly Explained**](https://towardsdatascience.com/glove-research-paper-clearly-explained-7d2c3641b8a6), by [**Meesala Lokesh**](https://medium.com/@meesala.lokesh)\n",
        "- [**Long Short-Term Memory**](https://www.bioinf.jku.at/publications/older/2604.pdf), by [**Sepp Hochreiter**](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [**Jurgen Schmidhuber**](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber)\n",
        "- [**The Unreasonable Effectiveness of Recurrent Neural Networks**](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), by [**Andrej Karpathy**](https://karpathy.ai/)\n",
        "- [**Understanding LSTM Networks**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), by [**Christopher Olah**](https://colah.github.io/about.html)"
      ],
      "metadata": {
        "id": "XTNY_ZAkBeoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime and memory usage\n",
        "stop = time.time()\n",
        "print(f\"Process runtime     : {float(stop - start):.2f} seconds\")\n",
        "print(f\"Process memory usage: {float(process.memory_info()[0]/(1024*1024)):.2f} MB\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T12:58:25.361729Z",
          "iopub.execute_input": "2023-05-12T12:58:25.362981Z",
          "iopub.status.idle": "2023-05-12T12:58:25.370361Z",
          "shell.execute_reply.started": "2023-05-12T12:58:25.362942Z",
          "shell.execute_reply": "2023-05-12T12:58:25.369152Z"
        },
        "trusted": true,
        "id": "rAYUKSJXBeoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}